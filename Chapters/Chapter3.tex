\doublespacing
\chap{Convolutional Neural Network} 
\label{chap:CNN}
\section{Introduction}
Deep learning, which is a branch of machine learning and artificial intelligence, which is  inspired by the structure of the human brain. It  has made enormous strides in giving machines the ability to intuit the physical world. Convolutional Neural Networks are modeled around how living beings perceive the world using their visual cortex. CNNs over the years have shown the ability to learn and extract complex features from images. In this work, we used this ability of convolutional neural networks to generate features. In this chapter, we will look at basics of a neural networks, starting with simple feed forward network and later delving into CNNs.


\section{Neural Network}

The very basic definition of an artificial neural network is a computer system modeled on the human brain and nervous system.
\par
In very abstract terms, a neural network contains various neurons each having multiple inputs and the importance of each input is given by its weight. A network of these neurons form a neural network. We now look at the basic building blocks of a neural network.

\subsection{Neuron}
In nature, a neuron consists of dendrites and  axons. The dendrites provide inputs to na euron from other neurons. After receiving the input, the neuron produces the output along the axons. The neuron sums up all the signals received from dendrites and activates based on some threshold value. 

\begin{figure}[H]
  \centering
    \includegraphics[scale=.3, angle=0]{Files/nn2.png}
    \caption[Single-layer Neural Networks (Perceptrons)]{Single-layer Neural Networks (Perceptrons)}
    \label{fig:NNNN}
\end{figure}


\par
Now that we are clear with the definition, we now explain fully feed forward neural networks.
\section{Feed Forward Neural Network}
The feed forward neural network is an extension of the above concept. It consists of multiple layers, each of which has multiple neurons and information flows from input to the output. An important characteristic of a feed forward neural network is that of not having any loop. Intuitively, a feed forward neural network is a digraph containing neurons as nodes and weighted edges. To train a feed forward neural network the most popular technique is backpropagation.
\begin{figure}[H]
  \centering
    \includegraphics[scale=.6, angle=0]{Files/FFNN.png}
    \caption[Feed Forward Neural Network]{Feed Forward Neural Network}
    \label{fig:FFNN}
\end{figure}

\section{Backpropagation}
 Backpropagation \cite{rumelhart1988learning} is an algosrithm to find the local minima of an error function. In a feed forward neural network we have neurons and edges with weights which need to be fine tuned based on the output and error function.  The backpropagation algorithm computes the loss and uses  optimization algorithms such as gradient descent to propagate error in the backward direction recursively. This causes change in the weights of the edges, which enables the whole network to adapt according to the data.  
\begin{figure}[H]
  \centering
    \includegraphics[scale=.4, angle=0]{Files/BackPropagation.png}
    \caption[Back Propagation Visualization]{Back Propagation Visualization \cite{backprop}}
    \label{fig:backpropogation}
\end{figure}

\section{Convolutional Neural Network}

Convolutional Neural Networks are a variant of multi layer perceptrons, inspired by the organization of the animal visual cortex. Convolutional Neural Networks have repetitive blocks of neurons also known as features or kernels extending across the space. These repetitive blocks share weights during training. CNNs contain various types of layers, out of which the important layers are  as follows:

\subsection{Convolution}

Convolution is a mathematical operation applied over two real valued arguments. For two functions $f(x)$ and $h(x)$, convolution operation over one dimension would be,

 \begin{equation}\label{eq:convolution-1d}
        \begin{aligned}
            g(x)=f(x) \ast h(x) = \int_{-\infty }^{\infty} f(s) \times h(x-s) ds
        \end{aligned}
\end{equation}

Here s is a dummy integration variable. 
From the CNN and image perspective, our image becomes f(x) and kernel or feature map becomes h(x). The feature map is slides across the image and as it slides we apply the convolution operation over the image to generate a output feature map. The values of these filters are fine tuned during training, but we have to decide the following key parameters commonly known as hyper-parameters before training.
\begin{itemize}
    \item  Depth corresponds to the number of filters or kernels mapped at any layer. In \cref{fig:CNN} we can see that the first layer has 4 feature maps.
    \item Stride corresponds to the number of steps we take when we slide a feature map over the output from previous layer. For example, when we have stride is set as 1 and 3, then we slide filter one step and three steps each time respectively.
\end{itemize}

\subsection{ Activation function}
Once the convolution operation has been performed we apply a non linear activation function such as tanh, RELU, leakyRELU, sigmoid on the feature map. The activation function takes input from the previous node and applies given mathematical operation over it. In \cref{fig:activation} illustrates commonly used activation functions in CNNS.

\begin{figure}[H]
  \centering
    \includegraphics[scale=.5, angle=0]{Files/activation-function.png}
    \caption[Different Type of Activation Functions]{Different Type of Activation Functions \cite{activation}}
    \label{fig:activation}
\end{figure}

\subsection{Pooling}
Pooling is one of the important building blocks of a CNN. The main aim of the pooling operation is the reduction of feature maps by using some functions such taking maximum or minimum or average value of a subregions. Based on the value of the receptive field, the pooling operation is performed over the feature map obtained from the previous layer. For example, for a receptive field of value 3 and pooling function as average, we average a max value from every $3 \times 3$ area of feature map.

\begin{figure}[H]
  \centering
    \includegraphics[scale=.4, angle=0]{Files/Pooling.png}
    \caption[Pooling Operation]{Computing output when $3 \times 3$ average pooling operation is performed \cite{1603.07285}}
    \label{fig:Pooling}
\end{figure}

\subsection{Fully Connected layer}
The last output layer is fully connected  over which various cost function such as sigmoid, soft-max are applied to generate the output.


\begin{figure}[H]
  \centering
    \includegraphics[scale=.6, angle=0]{Files/cnn-2.png}
    \caption[Convolutional Neural Network]{Convolutional Neural Network}
    \label{fig:CNN}
\end{figure}