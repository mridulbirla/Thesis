%*******10********20********30********40********50********60********70********80

% For all chapters, use the newdefined chap{} instead of chapter{}
% This will make the text at the top-left of the page be the same as the chapter

\chap{Related Work} % history of the jetpak
\doublespacing


In this chapter, we look at the previous work done in the field of generative models. Here first we look at different generative model. As our work is focused on generative adversarial neural network, we look at the various models under the umbrella of it. %There has been lot of research going on in area of generative models.One of the most proven models belongs to Restricted boltzzman machine and Deep Belief Net[cite]. The Genrative adverserial Networks[cite] are the newest model to join the family.
\section{Generative Models}

There has been lot of research going on in area of generative models.One of the most proven models belongs to Restricted boltzzman machine and Deep Belief Net[cite].


In the work of coupled GAN, the author used two pairs of Generators and discriminators. This works targets learning of multi-domain images. The main contribution of this work lies in learning of joint distribution of image in two different domains. In this work to restrict image generated by two generator to share higher semantics, the author constraints the two generator to share weights at the first layer. Since, the generator the decodes information from higher to lower semantics wheres as discriminator which a standard Convolution Neural Network decodes the images in lower to higher semantics. Hence ,the two discrimnator last layer weights are also shared. The major issue with coupled adversarial network is training since we have to tackle lot of issues in training a single GAN.
\par

In the work of ADAGAN author address the issue of model collapse with Generative adversarial network. Model collapse happens when you have very diverse data-set and your GAN is not able to learn the whole distribution and end up learning some part of it. So the author proposes using GAN in boosting algorithm This work shows promises of tackling the issue of higly diverse data-set by using weak GANs.
\par

In this work of Unrolled GAN, the author proposes solution for various issues in training GANs. 
In this work author uses unrolled optimization of discriminator objective function.  The optimal solution for a adverarial network will is $\theta^{*} =\left\{\theta^{*}_{G}, \theta^{*}_{D}\right\}$ has iterative solution using one of the optimization techniques such as SDG, Adam. Now, the author unrolls the discriminator K time to create a surrogate objective function for generator as shown in below equation 2.1. As we can see if K=0 the we have a standard GAN.
\begin{equation}
f_{k}{ \left( \theta _{g}, \theta _{d} \right) }=f \left( \theta _{g}, \theta _{d}^{K}{ \left( \theta _{g}, \theta _{d} \right) } \right)
\end{equation}

In this work, author proposes parallelization approach for tackling over-fitting and long training time problem with GAN. To explain over-fitting author uses analogy of a fighter and his trainer. In this, when a fighter is trained with a particular style of the trainer then in any fight the fighter will perform better when the style of opponent matches the trainer. Similarly if  we have a distribution with multiple modes and if discriminator is not able figure out the correct label then generator won't be motivated to over different modes. To tackle this problem, author uses multiple generator and discriminator where each pair of generator and discriminator share parameter which very unlike many another technique uses. Here to maintain same synergy across various pair, the discriminators are swapped across different pair after K updates.  One of major advantage of this work is that it can be applied to any GAN architecture.
%G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Computation,
%18(7):1527â€“1554, 2006