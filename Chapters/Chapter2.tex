
\chap{Related Work} 
\doublespacing


Image generation is a very diverse and constantly evolving area of computer vision. In last couple of years, generative neural networks have shown very promising results. In this chapter, we look at various generative model ranging from Boltzmann machine to the latest convolution neural network based generative models.
\section{Early Work}

There has been lot of research going on in area of generative models. One of the most proven models belongs to Restricted Boltzmann machine and Deep Belief Net[cite].

Boltzmann machine is a approach for estimating probability distribution function over a binary vectors. Boltzmann machine were one of the first neural network model which showed that they can learn internal representation of the data\cite{Boltzman-Wiki}. The models based on Boltzmann machine are energy based models. In 
In the work they use boltzmann machine to generate MNIST digits. The author uses persistent markov chain for estimating model expectations. In this method, author uses stochastic parameter approximation procedure with continuous decreasing learning rate. In this work author also uses variational learning for estimating the data-dependent expectations. This in turned help in faster convergence.

\par 

Deep Belief Network


\section{GAN Extensions}
In the work of coupled GAN\cite{Co-GAN}, the author used two pairs of Generators and discriminators. This works targets learning of multi-domain images. The main contribution of this work lies in learning of joint distribution of image in two different domains. In this work to restrict image generated by two generator to share higher semantics, the author constraints the two generator to share weights at the first layer. Since, the generator the decodes information from higher to lower semantics wheres as discriminator which a standard Convolution Neural Network decodes the images in lower to higher semantics. Hence ,the two discriminator last layer weights are also shared. The major issue with coupled adversarial network is training since we have to tackle lot of issues in training a single GAN.
\par

In the work of Stack GAN (SGAN)\cite{stacked-gan}, the author proposes top-down stack of GANs. Each generator takes higher representation and adds lower representation to it. Here authors for training  a adversarial network uses adversarial, entropy and conditional losses. By using entropy loss author tackles the issue of conditional model collapse. Conditional model collapse  is a  scenario	observed	when	the	GAN	starts	ignoring the noise variable(z). In this work to process all the variations for a image author uses multiple noise variable(z) in SGAN. The proposed model uses a bottom up discriminator as a encoder which at each level guides the generator in producing intermediary representation of an image.

In the work of LRGAN \cite{LR-GAN}, the author takes advantage of viewing image as layers. In this work author uses 2 kind of generators , one responsible for generating background and the other generating foreground. In this whole process background is generated ones but foreground is generated recursively with help of long term short memory neural network\cite{hochreiter1997long}.The purpose of long term short memory network is to  provides information to the foreground generator about what has been generated. Each time a foreground image is generated, it is copy and pasted on the previously stitched image after doing affine and spatial transformation\cite{1506.02025}. 



In the work of Information GAN, the author introduces new variable apart from noise in GAN framework. This term encourages high mutual information between generated samples and a small subset of latent variables. By this author forces high information content interesting aspects of the representation into the conditional variable. This work looked promising but when we tried to replicate the work, this model failed in producing consistent results with complex datasets


\section{Improving GAN}

In the work of ADAGAN, author address the issue of model collapse. Model collapse happens when you have very diverse data-set and your GAN is not able to learn the whole distribution and end up learning some part of it. So the author proposes using GAN in boosting algorithm. This work shows promises of tackling the issue of highly diverse data-set by using bunch of weak GANs.
\par

In this work of Unrolled GAN, the author proposes solution for various issues in training GANs. 
In this work author uses unrolled optimization of discriminator objective function.  The optimal solution for a adverarial network will is $\theta^{*} =\left\{\theta^{*}_{G}, \theta^{*}_{D}\right\}$ has iterative solution using one of the optimization techniques such as SDG, Adam. Now, the author unrolls the discriminator K time to create a surrogate objective function for generator as shown in below equation 2.1. As we can see if K=0 the we have a standard GAN.
\begin{equation}
f_{k}{ \left( \theta _{g}, \theta _{d} \right) }=f \left( \theta _{g}, \theta _{d}^{K}{ \left( \theta _{g}, \theta _{d} \right) } \right)
\end{equation}

In this work, author proposes parallelization approach for tackling over-fitting and long training time problem with GAN. To explain over-fitting author uses analogy of a fighter and his trainer. In this, when a fighter is trained with a particular style of the trainer then in any fight the fighter will perform better when the style of opponent matches the trainer. Similarly if  we have a distribution with multiple modes and if discriminator is not able figure out the correct label then generator won't be motivated to over different modes. To tackle this problem, author uses multiple generator and discriminator where each pair of generator and discriminator share parameter which very unlike many another technique uses. Here to maintain same synergy across various pair, the discriminators are swapped across different pair after K updates.  One of major advantage of this work is that it can be applied to any GAN architecture.
%G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Computation,
%18(7):1527â€“1554, 2006