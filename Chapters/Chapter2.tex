%*******10********20********30********40********50********60********70********80

% For all chapters, use the newdefined chap{} instead of chapter{}
% This will make the text at the top-left of the page be the same as the chapter

\chap{Related Work} % history of the jetpak
\doublespacing


Image generation is a very diverse and constantly evolving area of computer vision. In last couple of years, generative neural networks have shown very promising results. In this chapter, we look at various generative model ranging from Boltzmann machine to the latest convolution neural network based generative models.
\section{Generative Models}

There has been lot of research going on in area of generative models. One of the most proven models belongs to Restricted Boltzmann machine and Deep Belief Net[cite].

Boltzmann machine is a approach for estimating probability distribution function over a binary vectors. Boltzmann machine were one of the first neural network model which showed that they can learn internal representation of the data\cite{Boltzman-Wiki}. The models based on Boltzmann machine are energy based models. In 
In the work they use boltzmann machine to generate MNIST digits. The author uses persistent markov chain for estimating model expectations. In this method, author uses stochastic parameter approximation procedure with continuous decreasing learning rate. In this work author also uses variational learning for estimating the data-dependent expectations. This in turned help in faster convergence.

\par 

Deep Belief Network

In the work of [] they uses this concept and apply it over multimodal data. The main work of this paper is in representing multi modal data such image-text using Deep Boltzmann machine. 

\par
In the work of coupled GAN, the author used two pairs of Generators and discriminators. This works targets learning of multi-domain images. The main contribution of this work lies in learning of joint distribution of image in two different domains. In this work to restrict image generated by two generator to share higher semantics, the author constraints the two generator to share weights at the first layer. Since, the generator the decodes information from higher to lower semantics wheres as discriminator which a standard Convolution Neural Network decodes the images in lower to higher semantics. Hence ,the two discrimnator last layer weights are also shared. The major issue with coupled adversarial network is training since we have to tackle lot of issues in training a single GAN.
\par

In the work of ADAGAN author address the issue of model collapse with Generative adversarial network. Model collapse happens when you have very diverse data-set and your GAN is not able to learn the whole distribution and end up learning some part of it. So the author proposes using GAN in boosting algorithm This work shows promises of tackling the issue of higly diverse data-set by using weak GANs.
\par

In this work of Unrolled GAN, the author proposes solution for various issues in training GANs. 
In this work author uses unrolled optimization of discriminator objective function.  The optimal solution for a adverarial network will is $\theta^{*} =\left\{\theta^{*}_{G}, \theta^{*}_{D}\right\}$ has iterative solution using one of the optimization techniques such as SDG, Adam. Now, the author unrolls the discriminator K time to create a surrogate objective function for generator as shown in below equation 2.1. As we can see if K=0 the we have a standard GAN.
\begin{equation}
f_{k}{ \left( \theta _{g}, \theta _{d} \right) }=f \left( \theta _{g}, \theta _{d}^{K}{ \left( \theta _{g}, \theta _{d} \right) } \right)
\end{equation}

In this work, author proposes parallelization approach for tackling over-fitting and long training time problem with GAN. To explain over-fitting author uses analogy of a fighter and his trainer. In this, when a fighter is trained with a particular style of the trainer then in any fight the fighter will perform better when the style of opponent matches the trainer. Similarly if  we have a distribution with multiple modes and if discriminator is not able figure out the correct label then generator won't be motivated to over different modes. To tackle this problem, author uses multiple generator and discriminator where each pair of generator and discriminator share parameter which very unlike many another technique uses. Here to maintain same synergy across various pair, the discriminators are swapped across different pair after K updates.  One of major advantage of this work is that it can be applied to any GAN architecture.
%G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Computation,
%18(7):1527â€“1554, 2006