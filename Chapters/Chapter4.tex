%*******10********20********30********40********50********60********70********80

% For all chapters, use the newdefined chap{} instead of chapter{}
% This will make the text at the top-left of the page be the same as the chapter
\doublespacing
\chap{Generative Adversarial Network}
\label{chap:GAN}
\section{Introduction}

The basic building block of this work is the Generative Adversarial Network. Since the first paper published by Goodfellow \textit{et al.} \cite{Original-GAN}, there has been many recent advancements in generative adversarial networks, and has now become one of the most studied generative models. A lot of variations and different architectures have been developed after the first paper was published. In this chapter, we explain GAN from the very basics.

\section{Generative Model}
Generative adversarial networks works on the principle of maximum likelihood.
%[https://arxiv.org/pdf/1701.00160.pdf].
It means, when given a dataset, the model tries to provide a probability distribution of the sample data, parametrized by $\theta$. The generative models based on maximum likelihood can be based divided into explicit and implicit density model. Below \cref{fig: jordan} classifies various models based on the maximum likelihood principle.
%
\begin{figure}[ht]
    
    \includegraphics[scale=.6, angle=0]{Files/taxanomy.png}
    \caption[Taxonomy of Generative Model]{Taxonomy of Generative Model \cite{GanTut}}
    \label{fig: jordan}
\end{figure}

In the explicit model, also known as the prescribed probabilistic model, we explicitly define the distribution of random variables and specify the log likelihood function.
In the implicit model we do not need to define the density function \cite{1}. The model learns the function from the data and generates samples in a single step.
%SJ:Grammatical error

\section{The GAN  Concept}
Generative Adversarial Networks work by inter playing two deep artificial neural networks, a generator (G) and a discriminator (D). These networks play a min max game, where the generator tries to produce a fake image and discriminator tries to identify whether it is a fake or a real image, as illustrated in Figure 3.2. In the \cref{fig: GAN-Overview} the generator is provided with random noise to produce a fake data sample , whereas the discriminator is given both the real and fake images and tries to classify these data samples.
\begin{figure}[t]

  \centering
    \includegraphics[scale=.4, angle=0]{Files/gan-overview.png}
    \caption[GAN overview]{ GAN overview \cite{Gan-overview}}
    \label{fig: GAN-Overview}
\end{figure}
\newpage
\subsection{Mathematical Definition}
Mathematically, let x,  $\theta_{G}$  and $\theta_{D}$ be data variables, optimal hyper-parameters of the generator and discriminator model.
Given a latent variable z, drawn from a Gaussian distribution, the generator transforms this variable to a sample from the data. The discriminator then tries to estimate whether $x$ is from the data space $p_{data}$. The final objective function F becomes
$$ \substack{\min\\ G} \substack{ \max\\ D} F (\theta_{G}, \theta_{D}) = E_{x\sim p_{data}} [log (D (x; \theta_{D}))] + E_{z\sim N(0,I)}\log (1- D(G (z; \theta_{G}) ; \theta_{D}))$$
\subsection{Real World Example}
To understand GANs better, lets consider a real world analogy. Suppose G is a counterfeit artist who produces fake money and D is an undercover agent from the FBI who is acting as a buyer. The task of D is to differentiate the money. To master this skill, G produces a fake currency batch and sells it to D. If D identifies the batch then G updates its skill based on feedback from D. %SJ:feedback from G or D?
In this way, both go back and forth, until G has learnt the art of producing fake money.
\section{Algorithm}
In this section we provide a basic algorithm for GAN. Basically for each iteration, we sample k noise values and generate a data sample using the generator. Now we feed this mini-batch of generated data sample combined with real data to the discriminator. Then we update the parameters of the discriminator using the stochastic gradient descent algorithm. Since we cannot directly compute the loss for the generator, so we use the discriminator's gradient but in the opposite direction. The reason for going in reverse is because we are training the discriminator to accurately predict real samples whereas generator is being trained to produce synthetic data samples. \Cref{fig:GAN Algorithm} provides the actual algorithm.
\begin{figure}[!htb]

  \centering
    \includegraphics[scale=.4, angle=0]{Files/Algorithm.png}
    \caption[Vanilla GAN Algorithm]{ GAN Algorithm \cite{Gan-overview}}
    \label{fig:GAN Algorithm}
\end{figure}

\section{Analysis}

We can analyse the discriminator to see why GANs are effective. The basic idea is when we take the derivative of the equation in section 3.3.1 with respect to the discriminator function, then the final equation is the ratio of data distribution to the combined data distribution,

$$ D(x) =\frac{p_{data}}{p_{data} + p_{model}}$$

So basically we are learning the ratio data density and the model density. \Cref{fig: GAN Graphical Explanation} visualizes the overall approach. The green  line is $p_{model}$ which is generated by our generator G, the black line depicts our actual data distribution $p_{data}$, and the blue line is the discriminator differentiating real and generated data. The horizontal line z below the graph is the domain of the latent variable drawn from a uniform Gaussian %SJ:capital G?
distribution and x is the data domain. The arrow indicates G(z). 
\begin{figure}[H]
  \centering
    \includegraphics[scale=.4, angle=0]{Files/GAN-Visualize.png}
    \caption[GAN Algorithm Visualization]{ GAN Graphical Explanation \cite{Gan-overview}}
    \label{fig: GAN Graphical Explanation}
\end{figure}

\section{Images as Data}

As explained above we can use GANs to model the data. Now to use images as data, we have to look at the images from a statistical point of view.  Images can be defined using probability density function. The probability density function gives us the likelihood of the random variable, given the distribution. So if we have a large dataset of images then we can visualize these images as PDFs in higher dimension space.
%SJ:and or then in earlier sentence?
The complexity of the distribution depends upon the number of images. Thus, our generator tries to find the true PDF from the real world data and generates images from it.
%SJ:of or from?